**ğŸ“Œ Cricket Score Predictor â€“ Machine Learning Project**

This project predicts the final score of a cricket innings using machine learning.
It is built using Python, Scikit-Learn, XGBoost, and Streamlit for deployment.

ğŸ“**Project Structure**
ML_project/
â”‚â”€â”€ ML.ipynb                # Jupyter Notebook with all training code
â”‚â”€â”€ cricket_pipeline.pkl    # Saved final ML pipeline
â”‚â”€â”€ app.py                  # Streamlit web application
â”‚â”€â”€ README.md               # Project documentation
â”‚â”€â”€ data/                   # Dataset (CSV)


**ğŸ“Œ 1. Project Overview**

The goal of this project is to build a regression model that predicts final runs scored based on match conditions such as:

Batting team
Bowling team
Current score
Balls bowled
Wickets left
Runs in last 3 overs
Current run rate
City


**ğŸ“Š 2. Dataset Description**
| Feature           | Description                |
| ----------------- | -------------------------- |
| batting_team      | Team currently batting     |
| bowling_team      | Team bowling               |
| current_score     | Score at prediction moment |
| balls_bowled      | Total balls bowled so far  |
| wickets_left      | Wickets remaining          |
| runs_last_3_overs | Form indicator             |
| crr               | Current Run Rate           |
| city              | Match location             |
| final_score       | Target variable            |

https://www.kaggle.com/veeralakrishna/cricsheet-a-retrosheet-for-cricket?select=t20s
Link to access datasets taken for this project. To open dataset, Click on the link mentioned. There we can find multiple sections of cricket matches such as ipl, bbl, ODIs,Tests, T20s etc. Among these, click on T20s to see the project dataset.

**ğŸ§  3. Machine Learning Workflow**
**âœ” Data Cleaning**
1.Missing value handling
2.Type conversions

**âœ” Feature Engineering**
1.One-Hot Encoding for categorical columns
2.Scaling numeric columns

âœ” Algorithm Testing

**Models tested:**

**Model	Status**
| Model                | Status            |
| -------------------- | ----------------- |
| Linear Regression    | âŒ High error      |
| Decision Tree        | âŒ Overfitting     |
| Random Forest        | âœ” Good            |
| AdaBoost             | âœ” Moderate        |
| XGBoost              | âœ” Strong          |
| **Voting Regressor** | **â­â­ Best Model** |


**ğŸ† 4. Best Model: VOTING REGRESSOR**

The final model is an ensemble of three strong regressors:
**RandomForest + AdaBoost + XGBoost**


ğŸ“Œ Voting Regressor gave the lowest error and best generalization, making it the final choice.


**ğŸ§ª 5. Final Machine Learning Pipeline**

The final pipeline includes:

SimpleImputer â†’ handle missing numeric values
StandardScaler â†’ scale numeric features
OneHotEncoder â†’ convert teams/city to vectors
VotingRegressor â†’ final model



